[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatization' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/np' to '272'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/hostfile' to '../hostfiles/kautz/kautz16-2.txt'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/host-speed' to '100000000000f'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/display-timing' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/tmpdir' to '/tmp'
[12.526927] [smpi_utils/INFO] Simulated time: 12.5269 seconds. 

The simulation took 363.438 seconds (after parsing and platform setup)
346.737 seconds were actual computation of the application
[12.526927] [smpi_utils/INFO] More than 75% of the time was spent inside the application code.
You may want to use sampling functions or trace replay to reduce this.
Params:Data set size	1280000
Params:Batch per node	32
Params:Number of node	272
Params:Mini Batch size	8704.000000
Params:Iteration per epoch	148
Params:Number of  epoch	1
Params:Number of iteration	148
Layer 0 [3211264.000000,1728.000000,86704128.0]
Layer 1 [3211264.000000,36864.000000,1849688064.0]
Layer 2 [802816.000000,0.000000,205520896.0]
Layer 3 [1605632.000000,73728.000000,924844032.0]
Layer 4 [1605632.000000,147456.000000,1849688064.0]
Layer 5 [401408.000000,0.000000,205520896.0]
Layer 6 [802816.000000,294912.000000,924844032.0]
Layer 7 [802816.000000,589824.000000,1849688064.0]
Layer 8 [802816.000000,589824.000000,1849688064.0]
Layer 9 [200704.000000,0.000000,205520896.0]
Layer 10 [401408.000000,1179648.000000,924844032.0]
Layer 11 [401408.000000,2359296.000000,1849688064.0]
Layer 12 [401408.000000,2359296.000000,1849688064.0]
Layer 13 [100352.000000,0.000000,205520896.0]
Layer 14 [100352.000000,2359296.000000,462422016.0]
Layer 15 [100352.000000,2359296.000000,462422016.0]
Layer 16 [100352.000000,2359296.000000,462422016.0]
Layer 17 [25088.000000,0.000000,51380224.0]
Layer 18 [4096.000000,102760448.000000,102760448.0]
Layer 19 [4096.000000,16777216.000000,16777216.0]
Layer 20 [1000.000000,4096000.000000,4096000.0]
Total number of gradient 138344128
Start training	0.004422
Start FW	0.004433
End FW	5.269974
Start BW	5.269974
End BW	10.554716
Start Update	10.554716
End Update	10.569086
Start Communication	10.569086
End Communication	12.442114
End training	 12.526878
